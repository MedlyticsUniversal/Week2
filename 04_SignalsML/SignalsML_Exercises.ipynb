{"cells":[{"cell_type":"markdown","metadata":{"id":"9XZEvoDUdSKh","colab_type":"text","cell_id":"005b9ce587e54c7a9e0e0ed8f2f3fd95","deepnote_cell_type":"markdown"},"source":"# Machine Learning with Signals Exercises\n\nPer usual, we are going to start by downloading any data that we need and importing all of the Python packages we will need too.","block_group":"d01a1e5d3a71496d9eb82582d9e52094"},{"cell_type":"code","metadata":{"id":"3MIrMPcldSQZ","colab":{"height":51,"base_uri":"https://localhost:8080/"},"outputId":"45bca704-e970-42e1-9dee-e7c7193ed38d","colab_type":"code","cell_id":"adcdfd5203334dc3a5f91685f6ea4f75","deepnote_cell_type":"code"},"source":"# Install Graphviz library if it is not already installed\n!pip install graphviz \n\n# Clone the audio data repository (should cleanly fail if already downloaded)\n!git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git\n\n# Define the path to where the data are downloaded\ndata_dir = '/work/Week2/04_SignalsML/free-spoken-digit-dataset/recordings'","block_group":"4191348ad447473f9b186ac0e012c75c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dY2O4r5bW-j7","colab":{"height":71,"base_uri":"https://localhost:8080/"},"outputId":"0b48d12f-b988-4294-edf6-c06c83cc5e75","colab_type":"code","cell_id":"d302ece968604ee9851da9d6b2f7577f","deepnote_cell_type":"code"},"source":"# Import basic system and numerical packages\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\n\n# Import statistical and signal processing functions\nfrom scipy import signal\nimport scipy.stats.mstats as mstats\nfrom sklearn import metrics\n\n# Import the scikit-learn functions and models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\n\n# Import audio i/o and playing functions\nimport scipy.io.wavfile\nfrom IPython.display import Audio\n\n# Import plotting functionality\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport graphviz\nimport seaborn as sns\n\n# Setup the plotting preferencs\n%matplotlib inline\nmatplotlib.rcParams.update({'font.size': 16})","block_group":"9f612a9505374e0d8b8178ffd24e0d91","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DpBRUp2ubJC","colab_type":"text","cell_id":"0cbca5e79c8c45d59dc5f666d2ac597b","deepnote_cell_type":"markdown"},"source":"## Problem 1:\n$\\text{Ben Bitdiddle}$, an amateur data scientist, is working on a difficult classication problem given by his boss $\\text{Alyssa P. Hacker}$. Eventually, he comes up with a few features that he believes are the best of the best for his data and plots them below. Based on the visualization in the plot below, which feature is the most useful and which one is the least useful. Why? Is Ben really the best?\n\n![good_or_bad_features.jpg](https://raw.githubusercontent.com/MedlyticsUniversal/Data/main/Images/good_or_bad_features.jpg)","block_group":"56559c15a7e64610b187ec142219766b"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":6,"fromCodePoint":0}],"cell_id":"a8186f6fe22b408a8f9248c4560aa284","deepnote_cell_type":"text-cell-callout"},"source":"> Answer: ","block_group":"c98f229f953342c695a4ff364653acfb"},{"cell_type":"markdown","metadata":{"id":"TpWVDR_D0gYh","colab_type":"text","cell_id":"910e44f704c9445a97fd2116b95f4112","deepnote_cell_type":"markdown"},"source":"## Problem 2\n\nHere we are revisiting our dataset of recordings of spoken digits (0-9) but adding in some twists! Remember to visualize and really think about what features may or may not be useful!","block_group":"6c697e657a2c436db30e8e1cd24051d1"},{"cell_type":"code","metadata":{"id":"qvpcS7sshZzF","colab":{"height":105,"base_uri":"https://localhost:8080/"},"outputId":"66bfa79a-dc52-4e71-8628-5e8d44f7ad77","colab_type":"code","cell_id":"506d98141c744667b89312e381363523","deepnote_cell_type":"code"},"source":"# Let's double check that we got all of the files we expected\nfile_list = os.listdir(data_dir)\nprint('{} audio samples\\n'.format(len(file_list)))\n\n# List the first few files\nprint('Example files...')\nprint(file_list[0:10])","block_group":"6a7a2a5127444122b9552cb0d8e8a155","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPEORGagajex","colab_type":"text","cell_id":"9b2aeb0c7ecd4c7d890fd1c2d74ac4a1","deepnote_cell_type":"markdown"},"source":"### Feature Engineering\nAs we saw earlier, some features are more useful for classification than others. Here, we provide you with functions to extract the **\"spectral centroid\"** and **\"spectral flatness\"** features from an audio file, but we will also help you start building your own feature extraction functions.","block_group":"914481d5d291496eb60b7223a0d7fe8c"},{"cell_type":"markdown","metadata":{"id":"p4pwjmMHcjk5","colab_type":"text","cell_id":"82cba37b6cc54f9ba0a78e673fa931ad","deepnote_cell_type":"markdown"},"source":"#### Spectral Centroid\n\nThe [**spectral centroid**](https://en.wikipedia.org/wiki/Spectral_centroid) indicates the frequency at which the energy of a spectrum is centered upon. This is like a weighted mean:\n\n#### $f_c=\\frac{\\sum_kA(k)f(k)}{\\sum_kA(k)}$\nwhere $A(k)$ is the spectral magnitude at frequency bin $k$ and $f(k)$ is the frequency at bin $k$.","block_group":"54cf4c7aa04c4db4a7c06a209db6cc9c"},{"cell_type":"code","metadata":{"id":"ThhUeXOCYXAN","colab":{},"colab_type":"code","cell_id":"43ee3aa950124764bbd1b97f03d17eec","deepnote_cell_type":"code"},"source":"def spectral_centroid(ft, f_s):\n    \"\"\"\n    Computes the spectral centroid from the FT of a signal\n\n    :param ft: output of Fourier transform (i.e., np.fft.fft())\n    :param f_s: sampling frequency (or `sampling rate`) (in Hz)\n    \"\"\"\n    # Generate the frequencies associated with the Fourier transform values\n    num_samples = len(ft)\n    freqs = np.linspace(0, f_s, num_samples)\n\n    # Grab the magnitude of the relevant part of the Fourier transform values\n    freqs = freqs[0:num_samples//2]\n    magnitude = np.abs(ft[0:num_samples//2]) * (2/num_samples)\n    \n    # Compute the weighted frequency to get the specral centroid\n    spec_centroid = np.sum(magnitude*freqs)/np.sum(magnitude)\n\n    return spec_centroid","block_group":"0dc3a5bcc63b4ec2bf005438adce5ff1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"POEqXNK7lbUu","colab_type":"text","cell_id":"2012c01bd1494fe3883c9e444a9e9c29","deepnote_cell_type":"markdown"},"source":"### Spectral Flatness\n\nThe [**spectral flatness**](https://en.wikipedia.org/wiki/Spectral_flatness), also known as tonality coefficient or Wiener entropy, is a measure used in digital signal processing to characterize an audio spectrum. Spectral flatness is typically measured in decibels and provides a way to quantify how noise-like a sound is, as opposed to being tone-like.","block_group":"1a87f007a5c64159902bde080d4a00f7"},{"cell_type":"code","metadata":{"id":"LrpDbTp8bZWR","colab":{},"colab_type":"code","cell_id":"ed4bc88e642f4671a48e9658dc39569a","deepnote_cell_type":"code"},"source":"def spectral_flatness(ft):\n    \"\"\"\n    Computes the spectral flatness of the FT of a signal\n    \n    :param ft: output of Fourier transform (i.e., np.fft.fft())\n    \"\"\"\n    # Grab the magnitude of the relevant part of the Fourier transform values\n    num_samples = len(ft)\n    magnitude = abs(ft[0:num_samples//2]) * (2/num_samples)\n \n    # Spectral flatness is simply the geometric mean of the FT magnitude\n    # divided by the arithmetic mean of the FT magnitude\n    spec_flatness = mstats.gmean(magnitude)/np.mean(magnitude)\n\n    return spec_flatness","block_group":"2c892a229eeb49bc85b32588b3e0ef5b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blizQbTb8SU3","colab_type":"text","cell_id":"5cafe833f24541f39c5f5b0ead31bd96","deepnote_cell_type":"markdown"},"source":"### Spectral Roll-off\nIn this section, you are going to write the code to extract a new feature called **\"spectral roll-off.\"** This measure can be used to distinguish voice from non-voice audio and give a good idea of what frequencies contain the majority of the energy. Spectral roll-off can be computed from the following inequality:\n\n#### $\\sum^{r}_{k=0} A(k) \\geq 0.85 \\sum^{N}_{k=0} A(k) \\text{,}$\n\nwhere $A(k)$ is the magnitude of the Fourier transform at index $k$.\n\nThe idea is to find the frequency associated with $r$, the minimum index at which the sum of the magnitudes of the Fourier transform from $k=0$ up to $k=r$ is greater than or equal to $85\\% $ of the sum of the Fourier transform magnitudes from $k=0$ to $k=N$.","block_group":"11681c08de714bb0b82dd95402f1e9ec"},{"cell_type":"code","metadata":{"id":"RrrsRVURlr-m","colab":{},"colab_type":"code","cell_id":"fa194bceb1454e9f956e635b964dff99","deepnote_cell_type":"code"},"source":"def spectral_rolloff(ft, f_s):\n    # Generate the frequencies associated with the Fourier transform values\n    num_samples = 0 # <-- UPDATE THESE LINES\n    freqs = 0       # <--\n\n    # Grab the magnitude of the relevant part of the Fourier transform values\n    freqs = 0       # <-- UPDATE THESE LINES\n    magnitude = 0   # <--\n\n    # Compute the sum of *all* the Fourier transform magnitudes\n    whole_sum = 0   # <-- UPDATE THIS LINE\n\n    # Initialize a variable to hold the cumulative sum of the magnitudes of the\n    # Fourier transform as we loop over it.\n    cumulative_sum = 0.0\n\n    # Loop over each element in the fourier transform\n    for k, ft_mag in enumerate(magnitude):\n        # Add the current magnitude of the cumulative sum\n        cumulative_sum += 0     # <-- UPDATE THIS LINE\n\n        # Check if the cumulative sum is currently greater than 85% of the\n        # whole sum, and return the frequency at which this inequality first becomes true.\n        if cumulative_sum >= 0.85*whole_sum:\n            return freqs[k]","block_group":"cf9fba54ddc9485ebaa1cdeb2bcb9a29","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DGcKtBn8U99c","colab_type":"text","cell_id":"eae4d38c8e154a3c9dd812f0a74c773d","deepnote_cell_type":"markdown"},"source":"### Implement your own feature extractors\nAdd your own feature! We've added some common techniques based on popular audio signal processing methods. However, there are plenty out there. The links below contain just some information regarding a few possible features to use (including those we have already implemented).\n\nSource 1: https://musicinformationretrieval.com/spectral_features.html\n\nSource 2: https://www.cs.ccu.edu.tw/~wtchu/courses/2014f_MCA/Lectures/Lecture%209%20Audio%20and%20Music%20Analysis%202.pdf\n\nThere are lots of features to choose from, so pick a few that you think might help classify an audio clipâ€”or feel free to create your own ;-)\n\n##### A few possible features from these sources:\n- Spectral Bandwidth\n- Spectral Flux\n- Zero Crossing Rate\n- Spectral Contrast\n    - This one is tricky because it requires using some form of the \"spectrogram\" (not just the Fourier transform), but it also contains *much* more information and may be helpful for signal classification.\n\n##### Other ideas:\n- The \"Maximum Frequency Value\" shown in the tutorial stil exists. We are not using it here, but don't forget it exists.\n- Weighted standard deviation of the Fourier transform\n    - This would be analogous to the spectral centroid but for standard deviation.\n- Break up frequency into several \"frequency subbands,\" compute centroid within each subband, and use each of those centroid values as a feature.\n- Use the *number* of different peaks in the Fourier transform as a feature.\n> **The sky is the limit, so get creative!**","block_group":"bfa7aab8c4ba4278a98187b4c19adaa7"},{"cell_type":"code","metadata":{"id":"XfgICUuZVXjy","colab":{},"colab_type":"code","cell_id":"4f39a1d5dfe440f8af213c412a3e9ded","deepnote_cell_type":"code"},"source":"def feature1_extractor(ft, f_s):\n    \"\"\"The cool feature extractor I wrote\"\"\"\n    pass","block_group":"e42a367c924a452b98539477ea6bef41","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UWD7GhXWw3Rw","colab":{},"colab_type":"code","cell_id":"09f361491e1d45b4b22b148d29efffdc","deepnote_cell_type":"code"},"source":"def feature2_extractor(ft, f_s):\n    \"\"\"The even cooler feature extractor I wrote\"\"\"\n    pass","block_group":"8088407f8df343ec81f9dec7f2084042","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mPEZTcSkYnZ","colab_type":"text","cell_id":"579aca37280a488d94b0e30772005d44","deepnote_cell_type":"markdown"},"source":"### Load full dataset and extract features","block_group":"3b89cad08c7e4d3dbc3db680503d258a"},{"cell_type":"code","metadata":{"id":"C2nB-pLjoVlU","colab":{},"colab_type":"code","cell_id":"500755c11d2442ef94d536e9e33c4db7","deepnote_cell_type":"code"},"source":"# We can use the regular expression functionality (`re` package) to robustly\n# parse the name of each audio file to retrieve the digit spoken, the person\n# speaking, and the trial number of that digit-speaker pair.\n\n# The following lines create little \"string parsers\" to grab that information.\nre_digit = re.compile('\\d+_')\nre_speaker = re.compile('_[a-z]+_')\nre_trial = re.compile('_\\d.')\n\n# Load this information into a DataFrame and create columns to hold the features:\n# Spectral Centroid (SC)\n# Spectral Flatness (SF)\n##########################################################\n# ADD THE NAMES OF YOUR FEATURES TO THIS LIST OF COLUMNS #\n##########################################################\ndf = pd.DataFrame(columns=['digit','speaker', 'SC','SF'])\n\n# Loop over each audio file in the data directory\nfor audio_file in os.listdir(data_dir):\n    # Try to load the file and parse all of its contents.\n    # If something goes wrong, Python will execute the contents of the \"except\" codeblock.\n    try:\n        # Use the same string parsers from before and grab info from filename\n        digit = int(re.match(re_digit, audio_file)[0][:-1])\n        speaker = re.search(re_speaker, audio_file)[0][1:-1]\n\n        # Read in the audio file\n        full_path_to_audio_file = os.path.join(data_dir, audio_file)\n        (sample_rate, y) = scipy.io.wavfile.read(full_path_to_audio_file)\n        \n        # If the audio recording is in stereo (2-channels), just use one of them\n        if len(y.shape) == 2:\n            y = y[:,0]\n\n        # Grab the number of samples, compute the sample interval, and generate\n        # the time-stamps for each of the audio samples\n        num_samples = len(y)\n        sample_interval = 1.0/sample_rate\n        t = np.arange(0, num_samples/sample_rate, sample_interval)\n\n        # Compute the Fourier transform of the audio signal\n        ft = np.fft.fft(y)\n\n        # Calculate the audio features to be stored in the data frame\n        sc = spectral_centroid(ft, sample_rate)\n        sf = spectral_flatness(ft)\n        feature1 = feature1_extractor(ft, sample_rate)\n        feature2 = feature2_extractor(ft, sample_rate)\n        ### OPTIONAL: ADD MORE FEATURES HERE!\n        ###\n        ### NOTE: IF YOUR FEATURE EXTRACTOR RETURNS MORE THAN ONE VALUE, THEN\n        ### YOU WILL NEED TO SEPARATE THOSE OUT INTO INDIVIDUAL SCALAR VARIABLES\n\n        # Add the info for this file to our dataframe\n        feature_dict = {'digit':digit, 'speaker':speaker,'SC':sc,'SF':sf,\n                        'feature1':feature1, # <-- UPDATE FEATURE NAME\n                        'feature2':feature2} # <-- UPDATE FEATURE NAME\n                                             # OPTIONAL: EXTEND THE DICTIONARY\n                                             # WITH EVEN MORE FEATURES!\n        df = df.append(feature_dict, ignore_index=True)\n\n    except Exception as err:\n        # Something went wrong! =(\n        # Notify the user of the audio file that broke and what the error was\n        print(audio_file)\n        raise err","block_group":"0678a16023dc4f3abd2ba20084d219a6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1-LovEVqnh2","colab":{"height":629,"base_uri":"https://localhost:8080/"},"outputId":"fd87afdf-cea8-4c73-aa34-c5b5dd548087","colab_type":"code","cell_id":"e12d4efb87b94e20ae961ac616e31434","deepnote_cell_type":"code"},"source":"# Print the head of the DataFrame\nprint('df.head():\\n')\nprint(df.head(15))\n\n# Also print some summary information about the DataFrame\nprint('\\n\\ndf.info():\\n')\nprint(df.info())","block_group":"23cfc77115734e478b763c128261541c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSKTSban0xt7","colab_type":"text","cell_id":"5db296692708466b889a0f40eee98314","deepnote_cell_type":"markdown"},"source":"### Considering Your Features\n\nNow that we have successfully extracted a bunch of features from the audio files in this database, let's pause and examine the results.","block_group":"66449b64f0d043f9af4726f0bf6c185c"},{"cell_type":"code","metadata":{"id":"r_k_wABdu2YI","colab":{"height":1000,"base_uri":"https://localhost:8080/"},"outputId":"8e41c71f-9729-41ca-a701-74b7fd7ba9ec","colab_type":"code","cell_id":"beae5b1011aa4b20aaf8df1892b04a3d","deepnote_cell_type":"code"},"source":"# Visualize a pairplot of the spectral features. In this instance, color each\n# data point by the *person* speaking the digit to see if each *speaker* has a\n# distinguishable set of spectral characteristics (e.g., deep vs high voice)\ng = sns.pairplot(data=df, vars=['SC', 'SF', 'feature1', 'feature2'],\n                 hue='speaker', corner=True)\nplt.suptitle('Spectral Features (colored by speaker)', x=0.5, y=1.02)\n\n# Clean up the plot appearance\nplt.gcf().tight_layout()\n\n# Repeat this plot, but now color each datapoint by the *digit* being spoken to\n# see if each *word* has a distinguishable set of spectral characteristics\n# (e.g., does the buzzing 'z' in 'zero' cause a different set of characteristics\n# than the sharp 't' sound in 'two'?)\ng = sns.pairplot(data=df, vars=['SC', 'SF', 'feature1', 'feature2'],\n                 hue='digit', corner=True)\nplt.suptitle('Spectral Features (colored by digit)', x=0.5, y=1.02)\n\n# Clean up the plot appearance\nplt.gcf().tight_layout()","block_group":"755c758d90a34bf38cca9ebcfc104fad","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M1nwGspN8fm9","colab_type":"text","cell_id":"26820e7935e8428980479d14af6147c4","deepnote_cell_type":"markdown"},"source":"## Problem 3\n\nDoes it look like you have engineered some features that help separate the audio files by which digit was being spoken? What about by which person was speaking?","block_group":"3a80e558b48e43a893b81aad8fe962a5"},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":6,"fromCodePoint":0}],"cell_id":"abf400ef817a4386b3f3ac10d049974d","deepnote_cell_type":"text-cell-callout"},"source":"> Answer: ","block_group":"a1dbb0014c864ce4bd99c418aa809cf2"},{"cell_type":"markdown","metadata":{"id":"Fh-Q8Cd6755u","colab_type":"text","cell_id":"a598cebd96104decbecc41fe73573cf7","deepnote_cell_type":"markdown"},"source":"You can re-jigger your feature extractors however you want (or add more) and re-run the above cells to examine those effects on the feature space.","block_group":"b602079f1b0248b2a619fe32b08695ff"},{"cell_type":"markdown","metadata":{"id":"nT74fm4M1eV-","colab_type":"text","cell_id":"9cb678f7b5ae4702b99492b1d2b96569","deepnote_cell_type":"markdown"},"source":"# Train Machine Learning Algorithm\n\nThe following section perfectly parallels the content from the tutorial. We are simply copying it over here to the exercises so that you can see if your features helped classify the audio data.\n\nYou are not *quite* done yet though. There are some thoughts in the \"Conclusion\" section you may want to consider.","block_group":"cb1478193f524707a5f3368b10f68a27"},{"cell_type":"code","metadata":{"id":"UdTb80ywsnIJ","colab":{"height":51,"base_uri":"https://localhost:8080/"},"outputId":"35064947-e442-4bea-828d-18f76b423943","colab_type":"code","cell_id":"3323d3b3b1094cc1a1e17ea7172af623","deepnote_cell_type":"code"},"source":"# Select the name of the column to be used as the desired 'label' output\n# (For these files, it makes sense to use either 'speaker' or 'digit')\nlabel = 'speaker'\n\n# Create a list of features to be used in the classification process.\n# We use the 'set' object in Python to quickly remove the unwanted columns\n\n# NOTE: Add any columns you *do not* want to be included in the training\n# algorithm to the \"columns_to_remove\" variable.\ncolumns_to_remove = set([label, 'file', 'trial'])\nfeatures = set(df.columns) - columns_to_remove\nfeatures = list(features)\n\nprint('Classifying {} using...'.format(label))\nprint('features: {}'.format(features))","block_group":"46a4a0b073234f9f9d3e75189d97bae4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GnBZfexpwggY","colab":{},"colab_type":"code","cell_id":"86144bad596d4a3d90e9b7b505e5d86b","deepnote_cell_type":"code"},"source":"# Convert the data in the 'speaker', 'digit', and 'trial' columns of the\n# DataFrame into the 'Categorical' type.\ndf.speaker = pd.Categorical(df.speaker)\ndf.digit = pd.Categorical(df.digit)","block_group":"db337852232b448b92349d6b7a13c1d7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwFYJAyAwp_B","colab":{"height":238,"base_uri":"https://localhost:8080/"},"outputId":"70432729-2709-49af-e584-b3e0556ba716","colab_type":"code","cell_id":"159a83914d554ba08cccec2bcca2e43e","deepnote_cell_type":"code"},"source":"# If the speaker is in the set of features used to classify the audio files,\n# then the string speaker value should be converted to its numerical version.\nif 'speaker' in features:\n    # Add a column containing the numerically encoded version of speaker name\n    df['speaker_code'] = df.speaker.cat.codes\n\n    # Replace the 'speaker' feature with the 'speaker_code' feature\n    features.remove('speaker')\n    features.append('speaker_code')\n\n    # Double check that there are not duplicate entries in the list of features\n    # by converting to a `set` object (which automatically removes duplicate\n    # entries) then converting back to a `list` object.\n    features = list(set(features))\n\n# Update the user on what columns and content remain in the DataFrame\ndf.info()","block_group":"9de5b0fbf8874ac8ae381f922a1f24d0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V4CH6ZfdzEbA","colab_type":"text","cell_id":"854208affd6c476795236c48e233693f","deepnote_cell_type":"markdown"},"source":"Next, we split the data into training and test sets.","block_group":"228b571066af48eea991daf97e92d803"},{"cell_type":"code","metadata":{"id":"POmoLVIe1d2k","colab":{},"colab_type":"code","cell_id":"b85de6a7c4ae4bcaaf37d9e291dbeac6","deepnote_cell_type":"code"},"source":"# Split X and y into training and testing sets\n# NOTE: Using convention that X = features, y = label\nX_train, X_test, y_train, y_test = train_test_split(\n    df.loc[:,features], df[label], test_size=0.25,\n    random_state=0, stratify = df[label])","block_group":"1ca453464fdb40a9b9be015e4dbc3a08","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PYx9GHfizKaS","colab_type":"text","cell_id":"1664fbce40d041edb9d57b714b2ddcca","deepnote_cell_type":"markdown"},"source":"Now, with the correct training features and labels, we can train a machine learning classification model. To start, let's consider a decision tree.","block_group":"9591051ce8fb4ff98a9f7f94c620bd5e"},{"cell_type":"code","metadata":{"id":"wvVI50ci1eAH","colab":{},"colab_type":"code","cell_id":"f1213c6ec6884a56a5f7aa21dcf9323c","deepnote_cell_type":"code"},"source":"# Instantiate a decision tree model\nmodel = tree.DecisionTreeClassifier(max_depth=4)\n\n# Fit the model to the training data\nmodel.fit(X_train, y_train)","block_group":"068dad24c5de44628b1b9499b217b4ae","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l5TWpNhE1d9x","colab":{},"colab_type":"code","cell_id":"afd2081a22e744599f12f6fe9d0e443b","deepnote_cell_type":"code"},"source":"# Convert each class name into a string representation\nclass_names = [str(c) for c in model.classes_]\n\n# Generate the data to visualize the decision tree\ndot_data = tree.export_graphviz(model, out_file=None, \n                         feature_names=features,  \n                         class_names=class_names,  \n                         filled=True, rounded=True) \n\n# Visualize the tree from that exported data\ngraph = graphviz.Source(dot_data)  \ngraph","block_group":"6b7f89f8f4fc4d85abe975f9aea09b28","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vy9xs8UU4XTq","colab_type":"text","cell_id":"84a1ecb09d29425da47f80929f222f3d","deepnote_cell_type":"markdown"},"source":"## Evaluate on Test Data","block_group":"0a0d799319d944f082f7bff3f87a9bfe"},{"cell_type":"code","metadata":{"id":"P7IgU64x6QAY","colab":{},"colab_type":"code","cell_id":"ca96fdb11ea94132ad887db2788d1574","deepnote_cell_type":"code"},"source":"# Visualize the confusion matrix\ndef plot_cmatrix(cm,labels,title='Confusion Matrix'):\n    \"\"\"\n    Plot the confusion matrix for the classifier\n\n    :param cm: the actual confusion matrix\n    :param labels: the labels to add along the axes of the matrix\n    :param title: the title to place over the confusion matrix plot\n    \"\"\"\n    # Generate a new figure and axes instance\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # Display the confusion matrix (attempt to account for normalization)\n    row_totals = np.sum(cm, axis=1)\n    vmax_val = row_totals.max() if row_totals.max() > 1 else 1.0\n    ax_im = ax.imshow(cm, cmap='Reds', vmin=0, vmax=vmax_val)\n\n    # Annotate the figure\n    plt.title(title, fontsize=20)\n    fig.colorbar(ax_im)\n    ax.set_xticks(range(len(labels)))\n    ax.set_yticks(range(len(labels)))\n    ax.set_xticklabels(labels, fontsize=16, rotation=70)\n    ax.set_yticklabels(labels, fontsize=16)\n    plt.xlabel('Predicted', fontsize=16)\n    plt.ylabel('True', fontsize=16)\n    plt.show()","block_group":"a087dce654da471f90c3a11fb2faad3a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6nWEv1M4Xf9","colab":{},"colab_type":"code","cell_id":"1eb942c1dbcc4488917056c605ae0392","deepnote_cell_type":"code"},"source":"# Predict class label probabilities\nlabels=np.sort(y_test.unique())\ny_test_pred = model.predict(X_test)\n\ncm = metrics.confusion_matrix(y_test,y_test_pred, labels)\nplot_cmatrix(cm, labels)","block_group":"c19d0f1d28724872b2543139fa611c7f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_-B4Ay05qC2","colab_type":"text","cell_id":"1995615a69434545b51319bfdffd0134","deepnote_cell_type":"markdown"},"source":"# Conclusion\n\nFor consistency with the tutorial, we have used a decision tree to classify the data, but you could train a *different* classifier. In particular, do you recall if there is a classifier that may help separate features that are all mingled together like some of these features are?","block_group":"138f3aad89294c6fb48fd0be44993ff0"},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c2703ed7-f8b4-40f5-8dd1-77fe823e4d60' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SignalsML_Exercises.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[]},"vscode":{"interpreter":{"hash":"db3da3f361baf1113dc5160ff8c57cf22bdd1b4d834d98df35e37ddb9b3b9b90"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3.7.7 ('ML')"},"language_info":{"name":"python","version":"3.7.7"},"deepnote_notebook_id":"c4449b2bea3740338c2e7234137ad159","deepnote_execution_queue":[]}}